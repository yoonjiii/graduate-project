import os
import re
import json
from dotenv import load_dotenv
from openai import OpenAI
from kiwipiepy import Kiwi
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer, util
from collections import defaultdict
import numpy as np

load_dotenv()
kiwi = Kiwi()

model_name = "nlp04/korean_sentiment_analysis_kcelectra"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

client = OpenAI(
  api_key=os.getenv("OPENAI_API_KEY"),
)

POSITIVE_LABELS = {0, 1, 2, 3, 4}
NEGATIVE_LABELS = {7, 8, 9, 10}
NEUTRAL_LABELS = {5, 6}

def remove_symbols(text):
    text = re.sub(r"[ã„±-ã…ã…-ã…£]+", "", text)  # ã…‹ã…‹, ã… ã…  ë“± ì œê±°
    text = re.sub(r"[^\w\sê°€-í£.,!?]", "", text)  # ì´ëª¨í‹°ì½˜, ê¸°íƒ€ ê¸°í˜¸ ì œê±°
    return text.strip()

def insert_period(text): # ì¢…ê²°ì–´ë¯¸ ë’¤ì— ë¬¸ì¥ë¶€í˜¸ê°€ ì—†ëŠ” ê²½ìš°, ë§ˆì¹¨í‘œ ì¶”ê°€.
    return re.sub(r"(ë‹ˆë‹¤|ì–´ìš”|ì•„ìš”|í•´ìš”|ë„¤ìš”|êµ°ìš”|êµ¬ìš”|ê³ ìš”|ì—¬ìš”|ë ¤ìš”)(?=\s[^\.\!\?])", r"\1.", text)

def split_sentences(text):
    return [s.text.strip() for s in kiwi.split_into_sents(text)]

def map_to_sentiment(label_id):
    if label_id in POSITIVE_LABELS:
        return "positive"
    elif label_id in NEGATIVE_LABELS:
        return "negative"
    else:
        return "neutral"

def classify_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = F.softmax(outputs.logits, dim=1)
        label_id = torch.argmax(probs, dim=1).item()
        return map_to_sentiment(label_id)

def string_match(keyword, sentence):
    words = keyword.split()
    for word in words:
        for i in range(len(word) - 1):
            sub = word[i:i+2]
            if sub in sentence:
                return True
    return False

def find_keywords_in_review(reviews, keyword_groups):
    sbert_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
    keyword_to_reviews = defaultdict(set)
    keyword_vecs = {}
    keyword_dict = {}
    for group in keyword_groups:
        main_kw = group[0]
        keyword_vecs[main_kw] = [sbert_model.encode(kw, normalize_embeddings=True) for kw in group]
        keyword_dict[main_kw] = group[1:]

    for idx, review in enumerate(reviews):
        cleaned = remove_symbols(review)
        sentence = insert_period(cleaned)
        sentences = split_sentences(sentence)
        
        for sentence in sentences:
            sentence_vec = sbert_model.encode(sentence, normalize_embeddings=True)

            for main_kw, vec_list in keyword_vecs.items():
                scores = [util.cos_sim(vec, sentence_vec).item() for vec in vec_list]
                avg_score = sum(scores) / len(scores)
                max_score = max(scores)
                
                if (max_score >= 0.5 and avg_score >= 0.35)\
                    or any(string_match(alt_kw, sentence) for alt_kw in keyword_dict[main_kw]):
                    keyword_to_reviews[main_kw].add((sentence, round(avg_score, 2), idx))
    
    for kw in keyword_to_reviews:
        keyword_to_reviews[kw] = sorted(keyword_to_reviews[kw], key=lambda x: x[1], reverse=True)
    print(keyword_to_reviews)
    return keyword_to_reviews

def get_embedding(text: str, model="text-embedding-3-small") -> list:
    response = client.embeddings.create(
        input=[text],
        model=model
    )
    return response.data[0].embedding

def batch_get_embeddings(text_list: list[str], model="text-embedding-3-small", batch_size = 50) -> list:
    all_embeddings = []

    for i in range(0, len(text_list), batch_size):
        batch = text_list[i:i+batch_size]
        response = client.embeddings.create(
            input=batch,
            model=model
        )
        batch_embeddings = [item.embedding for item in response.data]
        all_embeddings.extend(batch_embeddings)
    return all_embeddings

def cosine_similarity(vec1, vec2):
    vec1, vec2 = np.array(vec1), np.array(vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return np.dot(vec1, vec2) / (norm1 * norm2)

def find_keywords_in_review_with_openai(reviews, keyword_groups):
    keyword_to_reviews = defaultdict(set)
    keyword_vecs = {}
    keyword_dict = {}

    for group in keyword_groups:
        main_kw = group[0]
        keyword_vecs[main_kw] = [get_embedding(kw) for kw in group]
        keyword_dict[main_kw] = group[:]

    sentence_info = [] #(sentence, idx)
    for idx, review in enumerate(reviews):
        cleaned = remove_symbols(review)
        #sentence = insert_period(cleaned)
        sentences = split_sentences(cleaned)
        for s in sentences:
            if len(s) < 4 and len(s.split()) <= 1:
                continue
            sentence_info.append((s, idx))

    all_sentences = [s for s, _ in sentence_info]
    all_embeddings = batch_get_embeddings(all_sentences)

    for (sentence, idx), sentence_vec in zip(sentence_info, all_embeddings):

        scores = [] #ê° í‚¤ì›Œë“œì˜ ë§¥ìŠ¤ê°’ì„ ì €ì¥í–ˆë‹¤ê°€, ë§ˆì§€ë§‰ì— ê·¸ì¤‘ ê°€ì¥ í° ê°’ì„ ê°€ì§€ëŠ” í‚¤ì›Œë“œì™€ ë§¤ì¹­.
        matched = [] #ìµœì¢… ë§¤ì¹­
        for main_kw, vec_list in keyword_vecs.items():
            max_score = max([cosine_similarity(vec, sentence_vec) for vec in vec_list])
            if max_score >= 0.3:
                scores.append((max_score, main_kw))
                if any(string_match(alt_kw, sentence) for alt_kw in keyword_dict[main_kw]): #ì´ ê²½ìš°, ë°”ë¡œ ìµœì¢… ë§¤ì¹­ì— ì¶”ê°€.
                    matched.append((max_score, main_kw))
        if scores:
            top_match = max(scores)
            if top_match not in matched:
                matched.append(top_match)
        
        for (max_score, main_kw) in matched:
            keyword_to_reviews[main_kw].add((sentence, round(max_score, 2), idx))
    
    return keyword_to_reviews

def review_sentiment_analysis(keyword_to_reviews, keywords):
    keyword_stats = {kw: {"positive": [], "negative": [], "neutral": []} for kw in keywords}
    for keyword, matched in keyword_to_reviews.items():
        for sentence, idx in matched:
            sentiment = classify_sentiment(sentence)  # ì˜ˆ: "positive"
            keyword_stats[keyword][sentiment].append((sentence, idx))
    return keyword_stats

def gpt_review_filtering(reviews, keyword_groups):
    main_keywords = [group[0] for group in keyword_groups]

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages = [
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ í™”ì¥í’ˆì˜ ì˜¨ë¼ì¸ ì‡¼í•‘ëª° ì‚¬ìš©ì ë¦¬ë·° ì „ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            },
            {
                "role": "user",
                "content": f"""
                    ì•„ë˜ëŠ” ì „ì²˜ë¦¬ê°€ í•„ìš”í•œ ì†Œë¹„ì ë¦¬ë·° ëª©ë¡ì´ë©°, ë¶ˆí•„ìš”í•œ ë‚´ìš©ì„ ì¡°ê¸ˆë§Œ í•„í„°ë§í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ê° ë¦¬ë·°ë§ˆë‹¤ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”:
                        **ì£¼ì˜: {main_keywords}ì— ìˆëŠ” í‚¤ì›Œë“œì™€ ì—°ê´€ëœ ë‚´ìš©ì´ ëˆ„ë½ë˜ì§€ ì•Šë„ë¡ í•œë‹¤.**
                        1. ì œí’ˆê³¼ ë¬´ê´€í•œ ë‚´ìš©(ì œí’ˆ ì‚¬ìš© ì „ ê¸°ëŒ€, ê°€ê²©/ë°°ì†¡/ë¸Œëœë“œ ê´€ë ¨ ì½”ë©˜íŠ¸ ë“±)ì„ ì œì™¸í•´ ì£¼ì„¸ìš”.
                        2. ì˜¤íƒˆì ë° ë¬¸ì¥ë¶€í˜¸ ì˜¤ë¥˜ ìˆ˜ì •.
                        3. ê° ë¦¬ë·°ëŠ” JSON êµ¬ì¡°ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”:
                        [
                          {{
                            "original": "ì›ë³¸ ë¦¬ë·° ë‚´ìš©",
                            "filtered": "í•„ìš” ë¬¸ì¥ë§Œ ë‚¨ì€ ë‚´ìš©"
                          }},
                          ...
                        ]
                    ë¦¬ë·° ëª©ë¡: {reviews}
                    """
            }
        ],
        temperature=0.5
    )

    reply = response.choices[0].message.content
    #print(reply)

    if reply.startswith("```json"):
        reply = reply.lstrip("```json").rstrip("```").strip()
    elif reply.startswith("```"):
        reply = reply.lstrip("```").rstrip("```").strip()

    reply = re.sub(r'\\U[0-9A-Fa-f]{8}', '', reply)

    try:
        filtered = json.loads(reply)

        with open("review_filtering.json", "w", encoding="utf-8") as f:
            json.dump(filtered, f, ensure_ascii=False, indent=4)

        return filtered
    except Exception as e:
        print("ë¦¬ë·°í•„í„°ë§ - GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:", e)
        with open("review_filtering.txt", "w", encoding="utf-8") as f:
            f.write(reply)
        return reviews  # fallback

def gpt_review_filtering_batched(reviews, keyword_groups, batch_size=20):
    main_keywords = [group[0] for group in keyword_groups]
    filtered = []

    for i in range(0, len(reviews), batch_size):
        batch = reviews[i:i + batch_size]

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ í™”ì¥í’ˆì˜ ì˜¨ë¼ì¸ ì‡¼í•‘ëª° ì‚¬ìš©ì ë¦¬ë·° ì „ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                },
                {
                    "role": "user",
                    "content": f"""
                        ì•„ë˜ëŠ” ì „ì²˜ë¦¬ê°€ í•„ìš”í•œ ì†Œë¹„ì ë¦¬ë·° ëª©ë¡ì…ë‹ˆë‹¤. ê° ë¦¬ë·°ë§ˆë‹¤ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”:
                        **ì£¼ì˜: {main_keywords}ì— ìˆëŠ” í‚¤ì›Œë“œì™€ ì—°ê´€ëœ ë‚´ìš©ì´ ëˆ„ë½ë˜ì§€ ì•Šë„ë¡ í•œë‹¤.**
                        1. ì œí’ˆê³¼ ë¬´ê´€í•œ ë‚´ìš©(ê¸°ëŒ€, ê°€ê²©, ë°°ì†¡ ë“±)ì„ ì œì™¸
                        2. ì˜¤íƒˆì ë° ë¬¸ì¥ë¶€í˜¸ ì˜¤ë¥˜ ìˆ˜ì •
                        3. ì§€ë‚˜ì¹œ êµ¬ì–´ì²´ ìˆ˜ì •
                        4. ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
                        ```json
                        [
                          {{
                            "original": "ì›ë³¸ ë¦¬ë·°",
                            "filtered": "í•„ìš” ë¬¸ì¥ë§Œ ë‚¨ì€ ë‚´ìš©"
                          }},
                          ...
                        ]
                        ```
                        ë¦¬ë·° ëª©ë¡: {batch}
                    """
                }
            ],
            temperature=0.5
        )

        reply = response.choices[0].message.content

        if reply.startswith("```json"):
            reply = reply.lstrip("```json").rstrip("```").strip()
        elif reply.startswith("```"):
            reply = reply.lstrip("```").rstrip("```").strip()

        reply = re.sub(r'\\U[0-9A-Fa-f]{8}', '', reply)

        try:
            batch_filtered = json.loads(reply)
            filtered.extend(batch_filtered)
        except Exception as e:
            print(f"Batch {i // batch_size + 1} íŒŒì‹± ì‹¤íŒ¨:", e)
            # with open(f"review_filtering_batch_{i}.txt", "w", encoding="utf-8") as f:
            #     f.write(reply)
            # filtered.extend([{"original": r, "filtered": r} for r in batch])  # fallback

    with open("review_filtering.json", "w", encoding="utf-8") as f:
        json.dump(filtered, f, ensure_ascii=False, indent=4)

    return filtered

def analyze_sentiment_by_keyword(keyword_to_reviews):
    """
    keyword_to_reviews: Dict[str, Set[Tuple[str, float, int]]]
    ë°˜í™˜: Dict[str, Dict[str, List[Tuple[str, float, int]]]]
        ì˜ˆ: keyword_stats["ìˆ˜ë¶„ê´‘íƒ"]["positive"] = [(ë¬¸ì¥, ì ìˆ˜, ì¸ë±ìŠ¤), ...]
    """
    keyword_stats = defaultdict(lambda: {"positive": [], "neutral": [], "negative": []})

    for keyword, review_set in keyword_to_reviews.items():
        for sentence, score, idx in review_set:
            sentiment = classify_sentiment(sentence)
            keyword_stats[keyword][sentiment].append((sentence, score, idx))

    return keyword_stats

def main():
    # ë¦¬ë·° ë¶ˆëŸ¬ì˜¤ê¸°
    product_N = "product_0"
    filename = product_N + ".json"

    with open(filename, "r") as f:
        product = json.load(f)   #productì˜ ìƒì„¸ì´ë¯¸ì§€ url, 100ê°œì˜ ë¦¬ë·°ê°€ ë“¤ì–´ìˆìŒ.

    reviews = []
    for review_obj in product['reviews']:
        reviews.append(review_obj["content"])
    
    # í‚¤ì›Œë“œ ë¶ˆëŸ¬ì˜¤ê¸°
    with open("highlighted_subjects.json", "r") as f:
        keyword_data = json.load(f)

    keyword_groups = [[item["keyword"]] + item["keyword_synonyms"] for item in keyword_data["features"]]

    filtering_output = gpt_review_filtering_batched(reviews, keyword_groups)

    # with open("review_filtering.json", "r") as f:
    #     filtering_output = json.load(f)

    filtered_reviews = [item["filtered"] for item in filtering_output if "filtered" in item]
    keyword_to_reviews = find_keywords_in_review_with_openai(filtered_reviews, keyword_groups)

    print(keyword_to_reviews)
    print()

    for keyword, review_set in keyword_to_reviews.items():
        sorted_reviews = sorted(review_set, key=lambda x: x[1], reverse=True)
        print(f"\nğŸ” í‚¤ì›Œë“œ: {keyword} (ì´ {len(sorted_reviews)} ë¬¸ì¥)")
        for i, (sentence, score, idx) in enumerate(sorted_reviews):
            print(f"  {i}. ({score}) [ë¦¬ë·° #{idx}] {sentence}")


        
if __name__ == "__main__":
    main()