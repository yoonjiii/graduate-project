import os
import re
import json
from dotenv import load_dotenv
from openai import OpenAI
from kiwipiepy import Kiwi
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer, util

load_dotenv()
kiwi = Kiwi()

model_name = "nlp04/korean_sentiment_analysis_kcelectra"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

client = OpenAI(
  api_key=os.getenv("OPENAI_API_KEY"),
)

POSITIVE_LABELS = {0, 1, 2, 3, 4}
NEGATIVE_LABELS = {7, 8, 9, 10}
NEUTRAL_LABELS = {5, 6}

def remove_symbols(text):
    text = re.sub(r"[ã„±-ã…ã…-ã…£]+", "", text)  # ã…‹ã…‹, ã… ã…  ë“± ì œê±°
    text = re.sub(r"[^\w\sê°€-í£.,!?]", "", text)  # ì´ëª¨í‹°ì½˜, ê¸°íƒ€ ê¸°í˜¸ ì œê±°
    return text.strip()

def insert_period(text): # ì¢…ê²°ì–´ë¯¸ ë’¤ì— ë¬¸ì¥ë¶€í˜¸ê°€ ì—†ëŠ” ê²½ìš°, ë§ˆì¹¨í‘œ ì¶”ê°€.
    return re.sub(r"(ë‹ˆë‹¤|ì–´ìš”|ì•„ìš”|í•´ìš”|ë„¤ìš”|êµ°ìš”|êµ¬ìš”|ê³ ìš”|ì—¬ìš”|ë ¤ìš”)(?=\s[^\.\!\?])", r"\1.", text)

def split_sentences(text):
    return [s.text.strip() for s in kiwi.split_into_sents(text)]

def map_to_sentiment(label_id):
    if label_id in POSITIVE_LABELS:
        return "positive"
    elif label_id in NEGATIVE_LABELS:
        return "negative"
    else:
        return "neutral"

def classify_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = F.softmax(outputs.logits, dim=1)
        label_id = torch.argmax(probs, dim=1).item()
        return map_to_sentiment(label_id)

def find_keywords_in_review(reviews, keywords):
    sbert_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
    keyword_stats = {kw: {"positive": [], "negative": [], "neutral": []} for kw in keywords}

    keywords_vec = [(kw, sbert_model.encode(kw, normalize_embeddings=True)) for kw in keywords]

    for review in reviews:
        sentence_vec = sbert_model.encode(review, normalize_embeddings=True)

        for keyword, vec in keywords_vec:
            sim_score = util.cos_sim(vec, sentence_vec)
            if sim_score >= 0.5:
                sentiment = classify_sentiment(review)
                keyword_stats[keyword][sentiment].append(review)
    return keyword_stats



def gpt_analyze_review(reviews, keywords):
    preprocessed_reviews = []
    for review_obj in reviews:
        content = review_obj["content"]
        cleaned = remove_symbols(content)
        sentence = insert_period(cleaned)
        sentences = split_sentences(sentence)
        preprocessed_reviews.extend(sentences)

    keyword_stats = find_keywords_in_review(preprocessed_reviews, keywords)

    # ì—¬ê¸°ì„œ keyword_statsì— ëŒ€í•´ì„œ ì¶”ê°€ ë¶„ì„ ì˜ˆì •.
    for keyword, sentiments in keyword_stats.items():
        total = sum(len(sentiments[sent]) for sent in sentiments)
        print(f"\nğŸ” í‚¤ì›Œë“œ: {keyword} (ì´ {total} ë¬¸ì¥)")
        for label, reviews in sentiments.items():
            print(f"  - {label.capitalize()}: {len(reviews)}ê°œ")
            for example in reviews[:1]:
                print(f"    ì˜ˆì‹œ: {example}")


def main():
    product_N = "product_0"
    filename = product_N + ".json"

    with open(filename, "r") as f:
        product = json.load(f)   #productì˜ ìƒì„¸ì´ë¯¸ì§€ url, 100ê°œì˜ ë¦¬ë·°ê°€ ë“¤ì–´ìˆìŒ.
    
    keywords = ['ìˆ˜ë¶„ê´‘íƒ', 'ì´ˆë°€ì°©ì»¤ë²„', 'ìŠ¤í‚¨ì¼€ì–´ íš¨ê³¼', 'ë‹¤ì–‘í•œ ì»¬ëŸ¬', 'ê°€ë²¼ìš´ í…ìŠ¤ì²˜', 'ì§€ì†ë ¥', 'í¸ë¦¬í•œ ì‚¬ìš©']
    if keywords:
        gpt_analyze_review(product['reviews'], keywords)
        
if __name__ == "__main__":
    main()